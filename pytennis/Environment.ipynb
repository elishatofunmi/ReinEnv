{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "from pygame.locals import *\n",
    "pygame.init()\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, xmin, xmax, ymin, ymax):\n",
    "        \"\"\"\n",
    "        xmin: 150,\n",
    "        xmax: 450, \n",
    "        ymin: 100, \n",
    "        ymax: 600\n",
    "        \"\"\"\n",
    "        \n",
    "        self.StaticDiscipline = {\n",
    "            'xmin': xmin, \n",
    "            'xmax': xmax, \n",
    "            'ymin': ymin, \n",
    "            'ymax': ymax\n",
    "        }\n",
    "\n",
    "    def network(self, xsource, ysource = 100, Ynew = 600, divisor = 50): #ysource will always be 100\n",
    "        \"\"\"\n",
    "        For Network A\n",
    "        ysource: will always be 100\n",
    "        xsource: will always be between xmin and xmax (static discipline)\n",
    "        \n",
    "        For Network B\n",
    "        ysource: will always be 600\n",
    "        xsource: will always be between xmin and xmax (static discipline)\n",
    "        \"\"\"\n",
    "        \n",
    "        while True:\n",
    "            ListOfXsourceYSource = []\n",
    "            Xnew = np.random.choice([i for i in range(self.StaticDiscipline['xmin'], self.StaticDiscipline['xmax'])], 1)\n",
    "            #Ynew = np.random.choice([i for i in range(self.StaticDiscipline['ymin'], self.StaticDiscipline['ymax'])], 1)\n",
    "\n",
    "            source = (xsource, ysource)\n",
    "            target = (Xnew[0], Ynew)\n",
    "\n",
    "            #Slope and intercept\n",
    "            slope = (ysource - Ynew)/(xsource - Xnew[0])\n",
    "            intercept = ysource - (slope*xsource)\n",
    "            if (slope != np.inf) and (intercept != np.inf):\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        #print(source, target)\n",
    "        # randomly select 50 new values along the slope between xsource and xnew (monotonically decreasing/increasing)\n",
    "        XNewList = [xsource]\n",
    "\n",
    "        if xsource < Xnew:\n",
    "            differences = Xnew[0] - xsource\n",
    "            increment = differences /divisor\n",
    "            newXval = xsource\n",
    "            for i in range(divisor):\n",
    "\n",
    "                newXval += increment\n",
    "                XNewList.append(int(newXval))\n",
    "        else:\n",
    "            differences = xsource - Xnew[0]\n",
    "            decrement = differences /divisor\n",
    "            newXval = xsource\n",
    "            for i in range(divisor):\n",
    "\n",
    "                newXval -= decrement\n",
    "                XNewList.append(int(newXval))\n",
    "                \n",
    "\n",
    "        #determine the values of y, from the new values of x, using y= mx + c\n",
    "        yNewList = []\n",
    "        for i in XNewList:\n",
    "            findy = (slope * i) + intercept#y = mx + c\n",
    "            yNewList.append(int(findy))\n",
    "\n",
    "        ListOfXsourceYSource = [(x, y) for x, y in zip(XNewList, yNewList)]\n",
    "\n",
    "        return XNewList, yNewList\n",
    "    \n",
    "    \n",
    "    \n",
    "    def DefaultToPosition(self, x1, x2 = 300, divisor = 50):\n",
    "        DefaultPositionA = 300\n",
    "        DefaultPositionB = 300\n",
    "        XNewList = []\n",
    "        if x1 < x2:\n",
    "            differences = x2 - x1\n",
    "            increment = differences /divisor\n",
    "            newXval = x1\n",
    "            for i in range(divisor):\n",
    "                newXval += increment\n",
    "                XNewList.append(int(np.floor(newXval)))\n",
    "\n",
    "        else:\n",
    "            differences = x1 - x2\n",
    "            decrement = differences /divisor\n",
    "            newXval = x1\n",
    "            for i in range(divisor):\n",
    "                newXval -= decrement\n",
    "                XNewList.append(int(np.floor(newXval)))\n",
    "        return XNewList\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pytennis:\n",
    "    def __init__(self, fps = 50):\n",
    "        self.net = Network(150,450,100,600)\n",
    "        \n",
    "        # Testing\n",
    "        self.net = Network(150, 450, 100, 600)\n",
    "        self.NetworkA = self.net.network(300, ysource=100, Ynew=600)  # Network A\n",
    "        self.NetworkB = self.net.network(200, ysource=600, Ynew=100)  # Network B\n",
    "        # NetworkA\n",
    "\n",
    "        # display test plot of network A\n",
    "        #sns.jointplot(NetworkA[0], NetworkA[1])\n",
    "\n",
    "        # display test plot of network B\n",
    "        #sns.jointplot(NetworkB[0], NetworkB[1])\n",
    "        \n",
    "        self.out = self.net.DefaultToPosition(250)\n",
    "        \n",
    "        \n",
    "        self.lastxcoordinate = 350\n",
    "        \n",
    "        pygame.init()\n",
    "        self.FPS = fps\n",
    "        self.fpsClock = pygame.time.Clock()\n",
    "        \n",
    "    def setWindow(self):\n",
    "\n",
    "        # set up the window\n",
    "        self.DISPLAYSURF = pygame.display.set_mode((600, 700), 0, 32)\n",
    "        pygame.display.set_caption('REINFORCEMENT LEARNING (Discrete Mathematics) - TABLE TENNIS')\n",
    "        # set up the colors\n",
    "        self.BLACK = ( 0,0,0)\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.RED= (255,0,0)\n",
    "        self.GREEN = ( 0, 255,0)\n",
    "        self.BLUE = ( 0,0, 255)\n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def display(self):\n",
    "        self.setWindow()\n",
    "        self.DISPLAYSURF.fill(self.WHITE)\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.GREEN, (150, 100, 300, 500))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.RED, (150, 340, 300, 20))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.BLACK, (0, 20, 600, 20))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.BLACK, (0, 660, 600, 20))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        return\n",
    "    \n",
    "    def evaluate_state_from_last_coordinate(self, c):\n",
    "        \"\"\"\n",
    "        cmax: 450\n",
    "        cmin: 150\n",
    "        \n",
    "        c definately will be between 150 and 450.\n",
    "        state0 - (150 - 179)\n",
    "        state1 - (180 - 209)\n",
    "        state2 - (210 - 239)\n",
    "        state3 - (240 - 269)\n",
    "        state4 - (270 - 299)\n",
    "        state5 - (300 - 329)\n",
    "        state6 - (330 - 359)\n",
    "        state7 - (360 - 389)\n",
    "        state8 - (390 - 419)\n",
    "        state9 - (420 - 450)\n",
    "        \"\"\"\n",
    "        if c >= 150 and c <=179:\n",
    "            return 0\n",
    "        elif c >= 180 and c <= 209:\n",
    "            return 1\n",
    "        elif c >=210 and c <= 239:\n",
    "            return 2\n",
    "        elif c >=240 and c <= 269:\n",
    "            return 3\n",
    "        elif c>= 270 and c<=299:\n",
    "            return 4\n",
    "        elif c >= 300 and c <= 329:\n",
    "            return 5\n",
    "        elif c >= 330 and c <= 359:\n",
    "            return 6\n",
    "        elif c >= 360 and c <= 389:\n",
    "            return 7\n",
    "        elif c >= 390 and c <= 419:\n",
    "            return 8\n",
    "        elif c >= 420 and c <= 450:\n",
    "            return 9\n",
    "        \n",
    "    def evaluate_action(self, action, expectedState):\n",
    "        if action == expectedState:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def randomVal(self, action):\n",
    "        \"\"\"\n",
    "        cmax: 450\n",
    "        cmin: 150\n",
    "        \n",
    "        c definately will be between 150 and 450.\n",
    "        state0 - (150 - 179)\n",
    "        state1 - (180 - 209)\n",
    "        state2 - (210 - 239)\n",
    "        state3 - (240 - 269)\n",
    "        state4 - (270 - 299)\n",
    "        state5 - (300 - 329)\n",
    "        state6 - (330 - 359)\n",
    "        state7 - (360 - 389)\n",
    "        state8 - (390 - 419)\n",
    "        state9 - (420 - 450)\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            val = np.random.choice([i for i in range(150, 180)])\n",
    "        elif action == 1:\n",
    "            val = np.random.choice([i for i in range(180, 210)])\n",
    "        elif action == 2:\n",
    "            val = np.random.choice([i for i in range(210, 240)])\n",
    "        elif action == 3:\n",
    "            val = np.random.choice([i for i in range(240, 270)])\n",
    "        elif action == 4:\n",
    "            val = np.random.choice([i for i in range(270, 300)])\n",
    "        elif action == 5:\n",
    "            val = np.random.choice([i for i in range(300, 330)])\n",
    "        elif action == 6:\n",
    "            val = np.random.choice([i for i in range(330, 360)])\n",
    "        elif action == 7:\n",
    "            val = np.random.choice([i for i in range(360, 390)])\n",
    "        elif action == 8:\n",
    "            val = np.random.choice([i for i in range(390, 420)])\n",
    "        else:\n",
    "            val = np.random.choice([i for i in range(420, 450)])\n",
    "        return val\n",
    "        \n",
    "    def stepA(self, action, count = 0):\n",
    "        #playerA should play\n",
    "        if count == 0:\n",
    "            #playerax = lastxcoordinate\n",
    "            self.NetworkA = self.net.network(self.lastxcoordinate, ysource = 100, Ynew = 600) #Network A\n",
    "            self.out = self.net.DefaultToPosition(self.lastxcoordinate)\n",
    "\n",
    "            #update lastxcoordinate\n",
    "\n",
    "            self.bally = self.NetworkA[1][count]\n",
    "            self.playerax = self.ballx\n",
    "            \n",
    "            \n",
    "#             soundObj = pygame.mixer.Sound('sound/sound.wav')\n",
    "#             soundObj.play()\n",
    "#             time.sleep(0.4)\n",
    "#             soundObj.stop()\n",
    "        else:\n",
    "            self.ballx = self.NetworkA[0][count]\n",
    "            self.bally = self.NetworkA[1][count]\n",
    "            \n",
    "            # move playerbx with respect to action \n",
    "            self.playerbx = self.randomVal(action)\n",
    "            self.playerax = self.out[count]\n",
    "            \n",
    "            \n",
    "        obs = self.evaluate_state_from_last_coordinate(int(self.lastxcoordinate)) # last state of the ball\n",
    "        reward = self.evaluate_action(action, obs)\n",
    "        done = True\n",
    "        info = ''\n",
    "\n",
    "\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    \n",
    "    def stepB(self, action, count):\n",
    "        #playerB can play\n",
    "        if count == 0:\n",
    "            #playerbx = lastxcoordinate\n",
    "            self.NetworkB = self.net.network(self.lastxcoordinate, ysource = 600, Ynew = 100) #Network B\n",
    "            self.out = self.net.DefaultToPosition(self.lastxcoordinate)\n",
    "\n",
    "            #update lastxcoordinate\n",
    "            self.bally = self.NetworkB[1][count]\n",
    "            self.playerbx = self.ballx\n",
    "\n",
    "#             soundObj = pygame.mixer.Sound('sound/sound.wav')\n",
    "#             soundObj.play()\n",
    "#             time.sleep(0.4)\n",
    "#             soundObj.stop()\n",
    "        else:\n",
    "            self.ballx = self.NetworkB[0][count]\n",
    "            self.bally = self.NetworkB[1][count]\n",
    "            self.playerbx = self.out[count]\n",
    "            self.playerax = self.randomVal(action)\n",
    "            \n",
    "        obs = self.evaluate_state_from_last_coordinate(int(self.lastxcoordinate)) # last state of the ball\n",
    "        reward = self.evaluate_action(action, obs)\n",
    "        done = True\n",
    "        info = ''\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        # diplay team players\n",
    "        self.PLAYERA = pygame.image.load('images/cap.jpg')\n",
    "        self.PLAYERA = pygame.transform.scale(self.PLAYERA, (50, 50))\n",
    "        self.PLAYERB = pygame.image.load('images/cap.jpg')\n",
    "        self.PLAYERB = pygame.transform.scale(self.PLAYERB, (50, 50))\n",
    "        self.ball = pygame.image.load('images/ball.png')\n",
    "        self.ball = pygame.transform.scale(self.ball, (15, 15))\n",
    "\n",
    "        self.playerax = 150\n",
    "        self.playerbx = 250\n",
    "        \n",
    "        self.ballx = 250\n",
    "        self.bally = 300\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        nextplayer = 'A'\n",
    "        #player A starts by playing with state 0\n",
    "        obs, reward, done, info = env.stepA(0)\n",
    "        state = obs\n",
    "        next_state = 0\n",
    "        \n",
    "        while True:\n",
    "            self.display()\n",
    "            if nextplayer == 'A':\n",
    "\n",
    "                if count == 0:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueA = AgentA.model.predict(state)\n",
    "                    action = AgentA.epsilon_greedy(q_valueA, iteration)\n",
    "                    \n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepA.step(action = action, count = count)\n",
    "                    next_state = obs\n",
    "                    \n",
    "                    # Let's memorize what just happened\n",
    "                    AgentA.replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "                    state = next_state\n",
    "                    \n",
    "                    \n",
    "                    count += 1\n",
    "                else:                    \n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueA = AgentA.model.predict(state)\n",
    "                    action = AgentA.epsilon_greedy(q_valueA, iteration)\n",
    "                    \n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepA.step(action = action, count = count)\n",
    "                    next_state = obs\n",
    "                    \n",
    "                    # Let's memorize what just happened\n",
    "                    AgentA.replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "                    state = next_state\n",
    "                    \n",
    "                    count += 1\n",
    "                if count == 49:\n",
    "                    count = 0\n",
    "                    \n",
    "                    # Sample memories and use the target DQN to produce the target Q-Value\n",
    "                    X_state_val, X_action_val, rewards, X_next_state_val, continues = (AgentA.sample_memories(batch_size))\n",
    "                    next_q_values = AgentA.model.predict(X_next_state_val)\n",
    "                    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "                    y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "\n",
    "                    # Train the online DQN\n",
    "                    AgentA.model.fit(X_state_val,X_next_state_val)\n",
    "                    \n",
    "                    nextplayer = 'B'\n",
    "                    #evaluate A\n",
    "                else:\n",
    "                    nextplayer = 'A'\n",
    "\n",
    "            else:\n",
    "\n",
    "                if count == 0:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueB = AgentB.model.predict(state)\n",
    "                    action = AgentB.epsilon_greedy(q_valueB, iteration)\n",
    "                    \n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepB.step(action = action, count = count)\n",
    "                    next_state = obs\n",
    "                    \n",
    "                    # Let's memorize what just happened\n",
    "                    AgentB.replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "                    state = next_state\n",
    "                    count += 1\n",
    "                else:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueB = AgentB.model.predict(state)\n",
    "                    action = AgentB.epsilon_greedy(q_valueB, iteration)\n",
    "                    \n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepB.step(action = action, count = count)\n",
    "                    next_state = obs\n",
    "                    \n",
    "                    # Let's memorize what just happened\n",
    "                    AgentB.replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "                    state = next_state\n",
    "                    count += 1\n",
    "                if count == 49:\n",
    "                    count = 0\n",
    "                    \n",
    "                    # Sample memories and use the target DQN to produce the target Q-Value\n",
    "                    X_state_val, X_action_val, rewards, X_next_state_val, continues = (AgentB.sample_memories(batch_size))\n",
    "                    next_q_values = AgentB.model.predict(X_next_state_val)\n",
    "                    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "                    y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "\n",
    "                    # Train the online DQN\n",
    "                    AgentB.model.fit(X_state_val,X_next_state_val)\n",
    "                    \n",
    "                    nextplayer = 'A'\n",
    "                    #evaluate B\n",
    "                else:\n",
    "                    nextplayer = 'B'\n",
    "\n",
    "            \n",
    "            #CHECK BALL MOVEMENT\n",
    "            self.DISPLAYSURF.blit(self.PLAYERA, (self.playerax, 50))\n",
    "            self.DISPLAYSURF.blit(self.PLAYERB, (self.playerbx, 600))\n",
    "            self.DISPLAYSURF.blit(self.ball, (self.ballx, self.bally))\n",
    "\n",
    "            #update last coordinate\n",
    "            self.lastxcoordinate = self.ballx \n",
    "\n",
    "            pygame.display.update()\n",
    "            self.fpsClock.tick(self.FPS)\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "\n",
    "                if event.type == QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "\n",
    "\n",
    "           \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis = pytennis(fps = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qwerty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "tennis.reset()\n",
    "tennis.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DQN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab831304a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab831304a8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab831304a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab831304a8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabe846e630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabe846e630>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabe846e630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabe846e630>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabe846e630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabe846e630>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabe846e630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fabe846e630>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab8339c358>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83280f28>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab83226780>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.keras import models, Sequential, layers\n",
    "\n",
    "\n",
    "def QNetwork(X_state, name):\n",
    "    \n",
    "    with tf.variable_scope(name, reuse = tf.AUTO_REUSE) as scope:\n",
    "    \n",
    "        hid1 = tf.layers.dense(X_state, 16, activation = 'relu')\n",
    "        hid2 = tf.layers.dense(hid1, 12 , activation= 'relu')\n",
    "        outputs = tf.layers.dense(hid2, 10, activation = 'softmax')\n",
    "\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                        scope = scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name\n",
    "    \n",
    "    \n",
    "X_stateA = tf.placeholder(tf.float32, shape = (None, 1), name = 'Xa')\n",
    "X_stateB = tf.placeholder(tf.float32, shape = (None, 1), name = 'Xb')\n",
    "\n",
    "\n",
    "online_q_valuesA, online_varsA = QNetwork(X_stateA, name= 'q_networks/onlineA')\n",
    "target_q_valuesA, target_varsA = QNetwork(X_stateA, name= 'q_networks/targetA')\n",
    "\n",
    "\n",
    "online_q_valuesB, online_varsB = QNetwork(X_stateB, name= 'q_networks/onlineB')\n",
    "target_q_valuesB, target_varsB = QNetwork(X_stateB, name= 'q_networks/targetB')\n",
    "\n",
    "\n",
    "copy_opsA = [target_varA.assign(online_varsA[var_nameA]) for var_nameA, target_varA in target_varsA.items()] \n",
    "copy_online_to_targetA = tf.group(*copy_opsA)\n",
    "\n",
    "copy_opsB = [target_varB.assign(online_varsB[var_nameB]) for var_nameB, target_varB in target_varsB.items()] \n",
    "copy_online_to_targetB = tf.group(*copy_opsB)\n",
    "\n",
    "\n",
    "X_actionA = tf.placeholder(tf.int32, shape = [None])\n",
    "X_actionB = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "\n",
    "q_valueA = tf.reduce_sum(target_q_valuesA * tf.one_hot(X_actionA, 10), \n",
    "                       axis = 1, keep_dims = True)\n",
    "\n",
    "q_valueB = tf.reduce_sum(target_q_valuesB * tf.one_hot(X_actionB, 10), \n",
    "                       axis = 1, keep_dims = True)\n",
    "\n",
    "ya = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "yb = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "errora = tf.abs(ya - q_valueA)\n",
    "clipped_errora = tf.clip_by_value(errora, 0.0, 1.0)\n",
    "linear_errora = 2 * (errora - clipped_errora)\n",
    "lossa = tf.reduce_mean(tf.square(clipped_errora) + linear_errora)\n",
    "\n",
    "errorb = tf.abs(yb - q_valueB)\n",
    "clipped_errorb = tf.clip_by_value(errorb, 0.0, 1.0)\n",
    "linear_errorb = 2 * (errorb - clipped_errorb)\n",
    "lossb = tf.reduce_mean(tf.square(clipped_errorb) + linear_errorb)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "momentum = 0.95\n",
    "global_stepa = tf.Variable(0, trainable=False, name='global_stepa')\n",
    "global_stepb = tf.Variable(0, trainable=False, name='global_stepb')\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "\n",
    "\n",
    "training_opa = optimizer.minimize(lossa, global_step=global_stepa)\n",
    "training_opb = optimizer.minimize(lossb, global_step=global_stepb)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "replay_memory_size = 500000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indices = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3],cols[4].reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action\n",
    "    \n",
    "    \n",
    "n_steps = 4000000 # total number of training steps\n",
    "training_start = 10000 # start training after 10,000 game iterations\n",
    "training_interval = 4 # run a training step every 4 game iterations\n",
    "save_steps = 1000 # save the model every 1,000 training steps\n",
    "copy_steps = 10000 # copy online DQN to target DQN every 10,000 training steps\n",
    "discount_rate = 0.99\n",
    "skip_start = 90 # Skip the start of every game (it's just waiting time).\n",
    "batch_size = 50\n",
    "iteration = 0 # game iterations\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True # env needs to be reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis = pytennis(fps = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + \".index\"):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if done: # game over, start again\n",
    "            obs = tennis.reset()\n",
    "            for skip in range(skip_start): # skip the start of each game\n",
    "                obs, reward, done, info = env.stepA(0)\n",
    "            state = obs\n",
    "            \n",
    "            \n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "        \n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = obs\n",
    "        \n",
    "        \n",
    "        # Let's memorize what just happened\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "        \n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # only train after warmup period and at regular intervals\n",
    "        \n",
    "        # Sample memories and use the target DQN to produce the target Q-Value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (sample_memories(batch_size))\n",
    "        next_q_values = target_q_values.eval(feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        \n",
    "        # Train the online DQN\n",
    "        training_op.run(feed_dict={X_state: X_state_val,X_action: X_action_val, y: y_val})\n",
    "        \n",
    "        # Regularly copy the online DQN to the target DQN\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "        # And save regularly\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)\n",
    "        \n",
    "        #show the game\n",
    "        tennis.render()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "from pygame.locals import *\n",
    "pygame.init()\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, xmin, xmax, ymin, ymax):\n",
    "        \"\"\"\n",
    "        xmin: 150,\n",
    "        xmax: 450, \n",
    "        ymin: 100, \n",
    "        ymax: 600\n",
    "        \"\"\"\n",
    "        \n",
    "        self.StaticDiscipline = {\n",
    "            'xmin': xmin, \n",
    "            'xmax': xmax, \n",
    "            'ymin': ymin, \n",
    "            'ymax': ymax\n",
    "        }\n",
    "\n",
    "    def network(self, xsource, ysource = 100, Ynew = 600, divisor = 50): #ysource will always be 100\n",
    "        \"\"\"\n",
    "        For Network A\n",
    "        ysource: will always be 100\n",
    "        xsource: will always be between xmin and xmax (static discipline)\n",
    "        \n",
    "        For Network B\n",
    "        ysource: will always be 600\n",
    "        xsource: will always be between xmin and xmax (static discipline)\n",
    "        \"\"\"\n",
    "        \n",
    "        while True:\n",
    "            ListOfXsourceYSource = []\n",
    "            Xnew = np.random.choice([i for i in range(self.StaticDiscipline['xmin'], self.StaticDiscipline['xmax'])], 1)\n",
    "            #Ynew = np.random.choice([i for i in range(self.StaticDiscipline['ymin'], self.StaticDiscipline['ymax'])], 1)\n",
    "\n",
    "            source = (xsource, ysource)\n",
    "            target = (Xnew[0], Ynew)\n",
    "\n",
    "            #Slope and intercept\n",
    "            slope = (ysource - Ynew)/(xsource - Xnew[0])\n",
    "            intercept = ysource - (slope*xsource)\n",
    "            if (slope != np.inf) and (intercept != np.inf):\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        #print(source, target)\n",
    "        # randomly select 50 new values along the slope between xsource and xnew (monotonically decreasing/increasing)\n",
    "        XNewList = [xsource]\n",
    "\n",
    "        if xsource < Xnew:\n",
    "            differences = Xnew[0] - xsource\n",
    "            increment = differences /divisor\n",
    "            newXval = xsource\n",
    "            for i in range(divisor):\n",
    "\n",
    "                newXval += increment\n",
    "                XNewList.append(int(newXval))\n",
    "        else:\n",
    "            differences = xsource - Xnew[0]\n",
    "            decrement = differences /divisor\n",
    "            newXval = xsource\n",
    "            for i in range(divisor):\n",
    "\n",
    "                newXval -= decrement\n",
    "                XNewList.append(int(newXval))\n",
    "                \n",
    "\n",
    "        #determine the values of y, from the new values of x, using y= mx + c\n",
    "        yNewList = []\n",
    "        for i in XNewList:\n",
    "            findy = (slope * i) + intercept#y = mx + c\n",
    "            yNewList.append(int(findy))\n",
    "\n",
    "        ListOfXsourceYSource = [(x, y) for x, y in zip(XNewList, yNewList)]\n",
    "\n",
    "        return XNewList, yNewList\n",
    "    \n",
    "    \n",
    "    \n",
    "    def DefaultToPosition(self, x1, x2 = 300, divisor = 50):\n",
    "        DefaultPositionA = 300\n",
    "        DefaultPositionB = 300\n",
    "        XNewList = []\n",
    "        if x1 < x2:\n",
    "            differences = x2 - x1\n",
    "            increment = differences /divisor\n",
    "            newXval = x1\n",
    "            for i in range(divisor):\n",
    "                newXval += increment\n",
    "                XNewList.append(int(np.floor(newXval)))\n",
    "\n",
    "        else:\n",
    "            differences = x1 - x2\n",
    "            decrement = differences /divisor\n",
    "            newXval = x1\n",
    "            for i in range(divisor):\n",
    "                newXval -= decrement\n",
    "                XNewList.append(int(np.floor(newXval)))\n",
    "        return XNewList\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DQN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential, layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.momentum = 0.95\n",
    "        self.eps_min = 0.1\n",
    "        self.eps_max = 1.0\n",
    "        self.eps_decay_steps = 2000000\n",
    "        self.replay_memory_size = 500\n",
    "        self.replay_memory = deque([], maxlen=self.replay_memory_size)\n",
    "        n_steps = 4000000 # total number of training steps\n",
    "        self.training_start = 10000 # start training after 10,000 game iterations\n",
    "        self.training_interval = 4 # run a training step every 4 game iterations\n",
    "        self.save_steps = 1000 # save the model every 1,000 training steps\n",
    "        self.copy_steps = 10000 # copy online DQN to target DQN every 10,000 training steps\n",
    "        self.discount_rate = 0.99\n",
    "        self.skip_start = 90 # Skip the start of every game (it's just waiting time).\n",
    "        self.batch_size = 100\n",
    "        self.iteration = 0 # game iterations\n",
    "        self.done = True # env needs to be reset\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        self.model = self.DQNmodel()\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "    def DQNmodel(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(1,), activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def sample_memories(self, batch_size):\n",
    "        indices = np.random.permutation(len(self.replay_memory))[:batch_size]\n",
    "        cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "        for idx in indices:\n",
    "            memory = self.replay_memory[idx]\n",
    "            for col, value in zip(cols, memory):\n",
    "                col.append(value)\n",
    "        cols = [np.array(col) for col in cols]\n",
    "        return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3],cols[4].reshape(-1, 1))\n",
    "\n",
    "\n",
    "    def epsilon_greedy(self, q_values, step):\n",
    "        self.epsilon = max(self.eps_min, self.eps_max - (self.eps_max-self.eps_min) * step/self.eps_decay_steps)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(10) # random action\n",
    "        else:\n",
    "            return np.argmax(q_values) # optimal action\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AgentA = DQN()\n",
    "AgentB = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import pygame\n",
    "\n",
    "class pytennis:\n",
    "    def __init__(self, fps = 50):\n",
    "        self.net = Network(150,450,100,600)\n",
    "        self.updateRewardA = 0\n",
    "        self.updateRewardB = 0\n",
    "        self.updateIter = 0\n",
    "        self.lossA = 0\n",
    "        self.lossB = 0\n",
    "        self.restart = False\n",
    "        \n",
    "        # Testing\n",
    "        self.net = Network(150, 450, 100, 600)\n",
    "        self.NetworkA = self.net.network(300, ysource=100, Ynew=600)  # Network A\n",
    "        self.NetworkB = self.net.network(200, ysource=600, Ynew=100)  # Network B\n",
    "        # NetworkA\n",
    "\n",
    "        # display test plot of network A\n",
    "        #sns.jointplot(NetworkA[0], NetworkA[1])\n",
    "\n",
    "        # display test plot of network B\n",
    "        #sns.jointplot(NetworkB[0], NetworkB[1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.out = self.net.DefaultToPosition(250)\n",
    "        \n",
    "        \n",
    "        self.lastxcoordinate = 350\n",
    "        \n",
    "        pygame.init()\n",
    "        self.BLACK = ( 0,0,0)\n",
    "        \n",
    "        self.myFontA = pygame.font.SysFont(\"Times New Roman\", 25)\n",
    "        self.myFontB = pygame.font.SysFont(\"Times New Roman\", 25)\n",
    "        self.myFontIter = pygame.font.SysFont('Times New Roman', 25)\n",
    "        \n",
    "        \n",
    "        self.FPS = fps\n",
    "        self.fpsClock = pygame.time.Clock()\n",
    "        \n",
    "    def setWindow(self):\n",
    "\n",
    "        # set up the window\n",
    "        self.DISPLAYSURF = pygame.display.set_mode((600, 700), 0, 32)\n",
    "        pygame.display.set_caption('REINFORCEMENT LEARNING (Discrete Mathematics) - TABLE TENNIS')\n",
    "        # set up the colors\n",
    "        self.BLACK = ( 0,0,0)\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.RED= (255,0,0)\n",
    "        self.GREEN = ( 0, 255,0)\n",
    "        self.BLUE = ( 0,0, 255)\n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def display(self):\n",
    "        self.setWindow()\n",
    "        self.DISPLAYSURF.fill(self.WHITE)\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.GREEN, (150, 100, 300, 500))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.RED, (150, 340, 300, 20))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.BLACK, (0, 20, 600, 20))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.BLACK, (0, 660, 600, 20))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        return\n",
    "    \n",
    "    def evaluate_state_from_last_coordinate(self, c):\n",
    "        \"\"\"\n",
    "        cmax: 450\n",
    "        cmin: 150\n",
    "        \n",
    "        c definately will be between 150 and 450.\n",
    "        state0 - (150 - 179)\n",
    "        state1 - (180 - 209)\n",
    "        state2 - (210 - 239)\n",
    "        state3 - (240 - 269)\n",
    "        state4 - (270 - 299)\n",
    "        state5 - (300 - 329)\n",
    "        state6 - (330 - 359)\n",
    "        state7 - (360 - 389)\n",
    "        state8 - (390 - 419)\n",
    "        state9 - (420 - 450)\n",
    "        \"\"\"\n",
    "        if c >= 150 and c <=179:\n",
    "            return 0\n",
    "        elif c >= 180 and c <= 209:\n",
    "            return 1\n",
    "        elif c >=210 and c <= 239:\n",
    "            return 2\n",
    "        elif c >=240 and c <= 269:\n",
    "            return 3\n",
    "        elif c>= 270 and c<=299:\n",
    "            return 4\n",
    "        elif c >= 300 and c <= 329:\n",
    "            return 5\n",
    "        elif c >= 330 and c <= 359:\n",
    "            return 6\n",
    "        elif c >= 360 and c <= 389:\n",
    "            return 7\n",
    "        elif c >= 390 and c <= 419:\n",
    "            return 8\n",
    "        elif c >= 420 and c <= 450:\n",
    "            return 9\n",
    "        \n",
    "    def evaluate_action(self, action, expectedState):\n",
    "        if action == expectedState:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def randomVal(self, action):\n",
    "        \"\"\"\n",
    "        cmax: 450\n",
    "        cmin: 150\n",
    "        \n",
    "        c definately will be between 150 and 450.\n",
    "        state0 - (150 - 179)\n",
    "        state1 - (180 - 209)\n",
    "        state2 - (210 - 239)\n",
    "        state3 - (240 - 269)\n",
    "        state4 - (270 - 299)\n",
    "        state5 - (300 - 329)\n",
    "        state6 - (330 - 359)\n",
    "        state7 - (360 - 389)\n",
    "        state8 - (390 - 419)\n",
    "        state9 - (420 - 450)\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            val = np.random.choice([i for i in range(150, 180)])\n",
    "        elif action == 1:\n",
    "            val = np.random.choice([i for i in range(180, 210)])\n",
    "        elif action == 2:\n",
    "            val = np.random.choice([i for i in range(210, 240)])\n",
    "        elif action == 3:\n",
    "            val = np.random.choice([i for i in range(240, 270)])\n",
    "        elif action == 4:\n",
    "            val = np.random.choice([i for i in range(270, 300)])\n",
    "        elif action == 5:\n",
    "            val = np.random.choice([i for i in range(300, 330)])\n",
    "        elif action == 6:\n",
    "            val = np.random.choice([i for i in range(330, 360)])\n",
    "        elif action == 7:\n",
    "            val = np.random.choice([i for i in range(360, 390)])\n",
    "        elif action == 8:\n",
    "            val = np.random.choice([i for i in range(390, 420)])\n",
    "        else:\n",
    "            val = np.random.choice([i for i in range(420, 450)])\n",
    "        return val\n",
    "        \n",
    "    def stepA(self, action, count = 0):\n",
    "        #playerA should play\n",
    "        if count == 0:\n",
    "            self.NetworkA = self.net.network(self.lastxcoordinate, ysource = 100, Ynew = 600) #Network A\n",
    "            self.bally = self.NetworkA[1][count]\n",
    "\n",
    "            if self.restart == True:\n",
    "                self.playerax = self.ballx\n",
    "            else:\n",
    "                self.playerax = self.randomVal(action)\n",
    "            \n",
    "            \n",
    "#             soundObj = pygame.mixer.Sound('sound/sound.wav')\n",
    "#             soundObj.play()\n",
    "#             time.sleep(0.4)\n",
    "#             soundObj.stop()\n",
    "       \n",
    "        else:\n",
    "            self.ballx = self.NetworkA[0][count]\n",
    "            self.bally = self.NetworkA[1][count]\n",
    "            \n",
    "            \n",
    "        obsOne = self.evaluate_state_from_last_coordinate(int(self.ballx)) # last state of the ball\n",
    "        obsTwo = self.evaluate_state_from_last_coordinate(int(self.playerbx)) #evaluate player bx\n",
    "        obs = obsTwo\n",
    "        reward = self.evaluate_action(obsOne, obsTwo)\n",
    "        done = True\n",
    "        info = ''\n",
    "\n",
    "\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    \n",
    "    def stepB(self, action, count):\n",
    "        #playerB can play\n",
    "        if count == 0:\n",
    "            self.NetworkB = self.net.network(self.lastxcoordinate, ysource = 600, Ynew = 100) #Network B\n",
    "\n",
    "            self.bally = self.NetworkB[1][count]\n",
    "            if self.restart == True:\n",
    "                self.playerbx = self.ballx\n",
    "            else:\n",
    "                self.playerbx = self.randomVal(action)\n",
    "\n",
    "#             soundObj = pygame.mixer.Sound('sound/sound.wav')\n",
    "#             soundObj.play()\n",
    "#             time.sleep(0.4)\n",
    "#             soundObj.stop()\n",
    "        else:\n",
    "            self.ballx = self.NetworkB[0][count]\n",
    "            self.bally = self.NetworkB[1][count]\n",
    "            \n",
    "        obsOne = self.evaluate_state_from_last_coordinate(int(self.ballx)) # last state of the ball\n",
    "        obsTwo = self.evaluate_state_from_last_coordinate(int(self.playerax)) #evaluate player bx\n",
    "        obs = obsTwo\n",
    "        reward = self.evaluate_action(obsOne, obsTwo)\n",
    "        done = True\n",
    "        info = ''\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def computeLossA(self, reward):\n",
    "        if reward == 0:\n",
    "            self.lossA += 1\n",
    "        else:\n",
    "            self.lossA += 0\n",
    "        return\n",
    "\n",
    "    def computeLossB(self, reward):\n",
    "        if reward == 0:\n",
    "            self.lossB += 1\n",
    "        else:\n",
    "            self.lossB += 0\n",
    "        return\n",
    "    \n",
    "    def render(self):\n",
    "        # diplay team players\n",
    "        self.PLAYERA = pygame.image.load('images/cap.jpg')\n",
    "        self.PLAYERA = pygame.transform.scale(self.PLAYERA, (50, 50))\n",
    "        self.PLAYERB = pygame.image.load('images/cap.jpg')\n",
    "        self.PLAYERB = pygame.transform.scale(self.PLAYERB, (50, 50))\n",
    "        self.ball = pygame.image.load('images/ball.png')\n",
    "        self.ball = pygame.transform.scale(self.ball, (15, 15))\n",
    "\n",
    "        self.playerax = 150\n",
    "        self.playerbx = 250\n",
    "        \n",
    "        self.ballx = 250\n",
    "        self.bally = 300\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        nextplayer = 'A'\n",
    "        #player A starts by playing with state 0\n",
    "        obs, reward, done, info = 0, False, False, ''\n",
    "        stateA = 0\n",
    "        stateB = 0\n",
    "        next_state = 0\n",
    "        \n",
    "        iterations = 20000\n",
    "        iteration = 0\n",
    "        restart = False\n",
    "        \n",
    "        while iteration < iterations:\n",
    "            self.display()\n",
    "            self.randNumLabelA = self.myFontA.render('A (Win): '+str(self.updateRewardA) + ', A(loss): '+str(self.lossA), 1, self.BLACK)\n",
    "            self.randNumLabelB = self.myFontB.render('B (Win): '+str(self.updateRewardB) + ', B(loss): '+ str(self.lossB), 1, self.BLACK)\n",
    "            self.randNumLabelIter = self.myFontIter.render('Iterations: '+str(self.updateIter), 1, self.BLACK)\n",
    "            if nextplayer == 'A':\n",
    "\n",
    "                if count == 0:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueA = AgentA.model.predict([stateA])\n",
    "                    actionA = AgentA.epsilon_greedy(q_valueA, iteration)\n",
    "                    \n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepA(action = actionA, count = count)\n",
    "                    next_stateA = actionA\n",
    "                    \n",
    "                    # Let's memorize what just happened\n",
    "                    AgentA.replay_memory.append((stateA, actionA, reward, next_stateA, 1.0 - done))\n",
    "                    stateA = next_stateA\n",
    "                    \n",
    "                    \n",
    "                else:                    \n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueA = AgentA.model.predict([stateA])\n",
    "                    actionA = AgentA.epsilon_greedy(q_valueA, iteration)\n",
    "        \n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepA(action = actionA, count = count)\n",
    "                    next_stateA = actionA\n",
    "                    \n",
    "                    # Let's memorize what just happened\n",
    "                    AgentA.replay_memory.append((stateA, actionA, reward, next_stateA, 1.0 - done))\n",
    "                    stateA = next_stateA\n",
    "                  \n",
    "                  \n",
    "                if count == 49:\n",
    "                    count = 0\n",
    "                    \n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueA = AgentA.model.predict([stateA])\n",
    "                    actionA = AgentA.epsilon_greedy(q_valueA, iteration)\n",
    "                    obs, reward, done, info = self.stepA(action = actionA, count = count)\n",
    "                    next_stateA = actionA\n",
    "\n",
    "                    self.updateRewardA += reward\n",
    "                    self.computeLossA(reward)\n",
    "                    \n",
    "                    # Let's memorize what just happened\n",
    "                    AgentA.replay_memory.append((stateA, actionA, reward, next_stateA, 1.0 - done))\n",
    "\n",
    "                    #restart the game if player A fails to get the ball, and let B start the game\n",
    "                    if reward == 0:\n",
    "                        self.restart = True\n",
    "                        time.sleep(0.5)\n",
    "                        nextplayer = 'B'\n",
    "                    else:\n",
    "                        self.restart = False\n",
    "                        \n",
    "                    # Sample memories and use the target DQN to produce the target Q-Value\n",
    "                    X_state_val, X_action_val, rewards, X_next_state_val, continues = (AgentA.sample_memories(AgentA.batch_size))\n",
    "                    next_q_values = AgentA.model.predict([X_next_state_val])\n",
    "                    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "                    y_val = rewards + continues * AgentA.discount_rate * max_next_q_values\n",
    "\n",
    "                    # Train the online DQN\n",
    "                    AgentA.model.fit(X_state_val,tf.keras.utils.to_categorical(X_next_state_val, num_classes=10), verbose = 0)\n",
    "                    \n",
    "                    nextplayer = 'B'\n",
    "                    self.updateIter += 1\n",
    "                    \n",
    "            \n",
    "                    #evaluate A\n",
    "                else:\n",
    "                    nextplayer = 'A'\n",
    "                    \n",
    "               \n",
    "                    \n",
    "               \n",
    "            else:\n",
    "\n",
    "                if count == 0:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueB = AgentB.model.predict([stateB])\n",
    "                    actionB = AgentB.epsilon_greedy(q_valueB, iteration)\n",
    "                    \n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepB(action = actionB, count = count)\n",
    "                    next_stateB = actionB\n",
    "                    \n",
    "                    # Let's memorize what just happened\n",
    "                    AgentB.replay_memory.append((stateB, actionB, reward, next_stateB, 1.0 - done))\n",
    "                    stateB = next_stateB\n",
    "                else:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueB = AgentB.model.predict([stateB])\n",
    "                    actionB = AgentB.epsilon_greedy(q_valueB, iteration)\n",
    "                    \n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepB(action = actionB, count = count)\n",
    "                    next_stateB = actionB\n",
    "                    \n",
    "                    # Let's memorize what just happened\n",
    "                    AgentB.replay_memory.append((stateB, actionB, reward, next_stateB, 1.0 - done))\n",
    "                    tateB = next_stateB\n",
    "    \n",
    "                \n",
    "                \n",
    "                if count == 49:\n",
    "                    count = 0\n",
    "                    \n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueB = AgentB.model.predict([stateB])\n",
    "                    actionB = AgentB.epsilon_greedy(q_valueB, iteration)\n",
    "                    \n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepB(action = actionB, count = count)\n",
    "                    next_stateB = actionB\n",
    "                    \n",
    "                    #Let's memorize what just happened\n",
    "                    AgentB.replay_memory.append((stateB, actionB, reward, next_stateB, 1.0 - done))\n",
    "                    stateB = next_stateB\n",
    "                    self.updateRewardB += reward\n",
    "                    self.computeLossB(reward)\n",
    "                    \n",
    "                    \n",
    "                    #restart the game if player A fails to get the ball, and let B start the game\n",
    "                    if reward == 0:\n",
    "                        self.restart = True\n",
    "                        time.sleep(0.5)\n",
    "                        nextplayer = 'A'\n",
    "                    else:\n",
    "                        self.restart = False\n",
    "                    \n",
    "                    # Sample memories and use the target DQN to produce the target Q-Value\n",
    "                    X_state_val, X_action_val, rewards, X_next_state_val, continues = (AgentB.sample_memories(AgentB.batch_size))\n",
    "                    next_q_values = AgentB.model.predict([X_next_state_val])\n",
    "                    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "                    y_val = rewards + continues * AgentB.discount_rate * max_next_q_values\n",
    "\n",
    "                    # Train the online DQN\n",
    "                    AgentB.model.fit(X_state_val,tf.keras.utils.to_categorical(X_next_state_val, num_classes=10), verbose = 0)\n",
    "                    \n",
    "                    nextplayer = 'A'\n",
    "                    self.updateIter += 1\n",
    "                    #evaluate B\n",
    "                else:\n",
    "                    nextplayer = 'B'\n",
    "\n",
    "                    \n",
    "            count += 1\n",
    "            iteration += 1\n",
    "            \n",
    "            \n",
    "            #CHECK BALL MOVEMENT\n",
    "            self.DISPLAYSURF.blit(self.PLAYERA, (self.playerax, 50))\n",
    "            self.DISPLAYSURF.blit(self.PLAYERB, (self.playerbx, 600))\n",
    "            self.DISPLAYSURF.blit(self.ball, (self.ballx, self.bally))\n",
    "            self.DISPLAYSURF.blit(self.randNumLabelA, (300, 630))\n",
    "            self.DISPLAYSURF.blit(self.randNumLabelB, (300, 40))\n",
    "            self.DISPLAYSURF.blit(self.randNumLabelIter, (50, 40))\n",
    "\n",
    "            #update last coordinate\n",
    "            self.lastxcoordinate = self.ballx \n",
    "\n",
    "            pygame.display.update()\n",
    "            self.fpsClock.tick(self.FPS)\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "\n",
    "                if event.type == QUIT:\n",
    "                    AgentA.model.save('AgentA.h5')\n",
    "                    AgentB.model.save('AgentB.h5')\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "\n",
    "\n",
    "           \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qwerty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "tennis = pytennis(fps = 50)\n",
    "tennis.reset()\n",
    "tennis.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state, action, reward, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
